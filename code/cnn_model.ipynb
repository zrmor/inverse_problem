{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from models import CNNClassifier, MatrixDataset, get_feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of C and kmeans labels:\n",
      " 1326, 1326\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_path = \"../data/\"\n",
    "\n",
    "all_C = np.load(data_path + 'all_C.npy') # 1326 input matrices of size 30x30\n",
    "# nMF_label = np.load(data_path + 'nMF_labels.npy') # 1326 output labels (0 or 1)\n",
    "kmeans_label = np.load(data_path + 'KMeans_labels_k3.npy') # 1326 output labels (0, 1 or 2)\n",
    "\n",
    "print(f\"Number of C and kmeans labels:\\n {len(all_C)}, {len(kmeans_label)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Cs and labels after removing those with kmeans zero labels:\n",
      " 1325, 1325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove C with zero kmean labels \n",
    "non_zero_indices = np.where(kmeans_label != 0)[0]\n",
    "all_C = all_C[non_zero_indices]\n",
    "kmeans_label = kmeans_label[non_zero_indices]\n",
    "\n",
    "print(f\"Number of Cs and labels after removing those with kmeans zero labels:\\n {len(all_C)}, {len(kmeans_label)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated kmeans_label: (array([0, 1], dtype=int32), array([795, 530]))\n"
     ]
    }
   ],
   "source": [
    "# Replace values in kmeans_label \n",
    "kmeans_label[kmeans_label == 1] = 0  # Replace 1 with 0\n",
    "kmeans_label[kmeans_label == 2] = 1  # Replace 2 with 1\n",
    "\n",
    "print(f\"Updated kmeans_label: {np.unique(kmeans_label, return_counts=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input matrices with kmeans label 0: 795\n",
      "Number of input matrices with kmeans label 1: 530\n"
     ]
    }
   ],
   "source": [
    "class_0_C = all_C[kmeans_label == 0] \n",
    "class_1_C = all_C[kmeans_label == 1] \n",
    "\n",
    "print(f\"Number of input matrices with kmeans label 0: {class_0_C.shape[0]}\")\n",
    "print(f\"Number of input matrices with kmeans label 1: {class_1_C.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split sizes:\n",
      "Train + Validation: 1060 samples\n",
      "Test: 265 samples\n",
      "\n",
      "Starting grid search with k-fold cross validation\n",
      "Parameter grid:\n",
      "{'learning_rate': [0.001], 'conv_channels': [[1, 32, 64, 128]], 'fc_units': [[1152, 128, 2]], 'dropout_rate': [0.2], 'batch_size': [128]}\n",
      "Number of folds: 10\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training with parameters:\n",
      "Learning rate: 0.001\n",
      "Conv channels: [1, 32, 64, 128]\n",
      "Fully connected layers: [1152, 128, 2]\n",
      "Dropout rate: 0.2\n",
      "Batch size: 128\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:xqzkcj60) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr_0.001_conv1_fc1152_dropout_0.2_batch_128</strong> at: <a href='https://wandb.ai/personlich/inverse_problem/runs/xqzkcj60' target=\"_blank\">https://wandb.ai/personlich/inverse_problem/runs/xqzkcj60</a><br/> View project at: <a href='https://wandb.ai/personlich/inverse_problem' target=\"_blank\">https://wandb.ai/personlich/inverse_problem</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241225_214728-xqzkcj60/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:xqzkcj60). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/zahra/Projects/ccnetlab/inverse_problem/code/wandb/run-20241225_214803-rh7ur1dv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/personlich/inverse_problem/runs/rh7ur1dv' target=\"_blank\">lr_0.001_conv[1, 32, 64, 128]_fc[1152, 128, 2]_dropout_0.2_batch_128</a></strong> to <a href='https://wandb.ai/personlich/inverse_problem' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/personlich/inverse_problem' target=\"_blank\">https://wandb.ai/personlich/inverse_problem</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/personlich/inverse_problem/runs/rh7ur1dv' target=\"_blank\">https://wandb.ai/personlich/inverse_problem/runs/rh7ur1dv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/10\n",
      "Train size: 954, Validation size: 106\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zahra/anaconda3/envs/inverse_problem/lib/python3.11/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | conv_layers | Sequential | 93.1 K | train\n",
      "1 | fc_layers   | Sequential | 147 K  | train\n",
      "---------------------------------------------------\n",
      "240 K     Trainable params\n",
      "0         Non-trainable params\n",
      "240 K     Total params\n",
      "0.964     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zahra/anaconda3/envs/inverse_problem/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zahra/anaconda3/envs/inverse_problem/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "/Users/zahra/anaconda3/envs/inverse_problem/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:04<00:00,  1.61it/s, v_num=r1dv]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:04<00:00,  1.60it/s, v_num=r1dv]\n",
      "Fold 1 best validation accuracy: 0.9906\n",
      "\n",
      "Fold 2/10\n",
      "Train size: 954, Validation size: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/zahra/anaconda3/envs/inverse_problem/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/zahra/Projects/ccnetlab/inverse_problem/code/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | conv_layers | Sequential | 93.1 K | train\n",
      "1 | fc_layers   | Sequential | 147 K  | train\n",
      "---------------------------------------------------\n",
      "240 K     Trainable params\n",
      "0         Non-trainable params\n",
      "240 K     Total params\n",
      "0.964     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.69it/s, v_num=r1dv]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  4.67it/s, v_num=r1dv]\n",
      "Fold 2 best validation accuracy: 0.9717\n",
      "\n",
      "Fold 3/10\n",
      "Train size: 954, Validation size: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | conv_layers | Sequential | 93.1 K | train\n",
      "1 | fc_layers   | Sequential | 147 K  | train\n",
      "---------------------------------------------------\n",
      "240 K     Trainable params\n",
      "0         Non-trainable params\n",
      "240 K     Total params\n",
      "0.964     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.63it/s, v_num=r1dv]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.62it/s, v_num=r1dv]\n",
      "Fold 3 best validation accuracy: 1.0000\n",
      "\n",
      "Fold 4/10\n",
      "Train size: 954, Validation size: 106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name        | Type       | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | conv_layers | Sequential | 93.1 K | train\n",
      "1 | fc_layers   | Sequential | 147 K  | train\n",
      "---------------------------------------------------\n",
      "240 K     Trainable params\n",
      "0         Non-trainable params\n",
      "240 K     Total params\n",
      "0.964     Total estimated model params size (MB)\n",
      "20        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99:  25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  3.32it/s, v_num=r1dv]        "
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get my API key from environment variable first\n",
    "wandb_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "# If not found in environment, prompt user\n",
    "if not wandb_key:\n",
    "    wandb_key = getpass(\"Enter your Weights & Biases API key: \")\n",
    "\n",
    "wandb.login(key=wandb_key)\n",
    "\n",
    "# First split into train+val and test sets (80-20 split)\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    all_C, kmeans_label, test_size=0.2, random_state=42, stratify=kmeans_label\n",
    ")\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-3],\n",
    "    'conv_channels': [[1, 32, 64, 128]],\n",
    "    'fc_units': [[1152, 128, 2]],\n",
    "    'dropout_rate': [0.2],\n",
    "    'batch_size': [128]\n",
    "}\n",
    "\n",
    "# K-fold setup for cross-validation on training data\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search with k-fold cross validation\n",
    "best_val_acc = 0\n",
    "best_params = None\n",
    "\n",
    "print(\"Data split sizes:\")\n",
    "print(f\"Train + Validation: {len(X_trainval)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\\n\")\n",
    "\n",
    "print(\"Starting grid search with k-fold cross validation\")\n",
    "print(f\"Parameter grid:\\n{param_grid}\")\n",
    "print(f\"Number of folds: {kfold.n_splits}\\n\")\n",
    "\n",
    "# Grid search\n",
    "for lr in param_grid['learning_rate']:\n",
    "    for conv in param_grid['conv_channels']:\n",
    "        for fc in param_grid['fc_units']:\n",
    "            for dropout in param_grid['dropout_rate']:\n",
    "                for batch_size in param_grid['batch_size']:\n",
    "                    print(\"\\n\" + \"=\"*80)\n",
    "                    print(f\"Training with parameters:\")\n",
    "                    print(f\"Learning rate: {lr}\")\n",
    "                    print(f\"Conv channels: {conv}\")\n",
    "                    print(f\"Fully connected layers: {fc}\")\n",
    "                    print(f\"Dropout rate: {dropout}\")\n",
    "                    print(f\"Batch size: {batch_size}\\n\")\n",
    "\n",
    "                    fold_scores = []\n",
    "                    \n",
    "                    # Initialize a new wandb run for this parameter combination\n",
    "                    run = wandb.init(\n",
    "                        project=\"inverse_problem\",\n",
    "                        name=f\"lr_{lr}_conv{conv}_fc{fc}_dropout_{dropout}_batch_{batch_size}\",\n",
    "                        config={\n",
    "                            \"learning_rate\": lr,\n",
    "                            \"conv_channels\": conv,\n",
    "                            \"fc_units\": fc,\n",
    "                            \"dropout_rate\": dropout,\n",
    "                            \"batch_size\": batch_size\n",
    "                        },\n",
    "                        reinit=True\n",
    "                    )\n",
    "                    \n",
    "                    # K-fold cross validation\n",
    "                    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_trainval)):\n",
    "                        print(f\"\\nFold {fold+1}/{kfold.n_splits}\")\n",
    "                        print(f\"Train size: {len(train_idx)}, Validation size: {len(val_idx)}\")\n",
    "                        \n",
    "                        # Prepare data for this fold\n",
    "                        X_train, X_val = X_trainval[train_idx], X_trainval[val_idx]\n",
    "                        y_train, y_val = y_trainval[train_idx], y_trainval[val_idx]\n",
    "                        \n",
    "                        train_dataset = MatrixDataset(X_train, y_train)\n",
    "                        val_dataset = MatrixDataset(X_val, y_val)\n",
    "                        \n",
    "                        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "                        \n",
    "                        # Initialize model and trainer\n",
    "                        model = CNNClassifier(\n",
    "                            learning_rate=lr,\n",
    "                            conv_channels=conv,\n",
    "                            fc_units=fc,\n",
    "                            dropout_rate=dropout\n",
    "                        )\n",
    "                        \n",
    "                        checkpoint_callback = ModelCheckpoint(\n",
    "                            monitor='val_acc',\n",
    "                            dirpath='checkpoints/',\n",
    "                            filename=f'fold{fold}_lr{lr:.4f}_conv{conv}_fc{fc}_dropout{dropout:.2f}_batch{batch_size}.ckpt',\n",
    "                            save_top_k=1,\n",
    "                            mode='max'\n",
    "                        )\n",
    "                        \n",
    "                        trainer = pl.Trainer(\n",
    "                            max_epochs=100,\n",
    "                            logger=WandbLogger(project=\"inverse_problem\"),\n",
    "                            callbacks=[checkpoint_callback],\n",
    "                            accelerator='auto'\n",
    "                        )\n",
    "\n",
    "                        # Train model\n",
    "                        print(\"Training model...\")\n",
    "                        trainer.fit(model, train_loader, val_loader)\n",
    "                        \n",
    "                        fold_score = checkpoint_callback.best_model_score.item()\n",
    "                        fold_scores.append(fold_score)\n",
    "                        print(f\"Fold {fold+1} best validation accuracy: {fold_score:.4f}\")\n",
    "                    \n",
    "                    # Calculate average score for this parameter combination\n",
    "                    avg_score = np.mean(fold_scores)\n",
    "                    print(\"\\nResults for current parameters:\")\n",
    "                    print(f\"Average validation accuracy: {avg_score:.4f}\")\n",
    "                    print(f\"Standard deviation: {np.std(fold_scores):.4f}\")\n",
    "                    \n",
    "                    # Update best parameters if necessary\n",
    "                    if avg_score > best_val_acc:\n",
    "                        best_val_acc = avg_score\n",
    "                        best_params = {\n",
    "                            'learning_rate': lr,\n",
    "                            'conv_channels': conv,\n",
    "                            'fc_units': fc,\n",
    "                            'dropout_rate': dropout,\n",
    "                            'batch_size': batch_size\n",
    "                        }\n",
    "                        torch.save(model.state_dict(), f'best_model_lr{lr:.4f}_conv{conv}_fc{fc}_dropout{dropout:.2f}_batch{batch_size}.pth')  # Save the model state\n",
    "                        print(\"\\nðŸŒŸ New best model found!\")\n",
    "                        print(f\"Best validation accuracy so far: {best_val_acc:.4f}\")\n",
    "                    \n",
    "                    wandb.finish()\n",
    "                    print(\"\\nFinished wandb run\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Grid search completed!\")\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(f\"Learning rate: {best_params['learning_rate']}\")\n",
    "print(f\"Conv channels: {best_params['conv_channels']}\")\n",
    "print(f\"Fully connected layers: {best_params['fc_units']}\")\n",
    "print(f\"Dropout rate: {best_params['dropout_rate']}\")\n",
    "print(f\"Batch size: {best_params['batch_size']}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Train final model with best parameters on all training data\n",
    "print(\"\\nTraining final model with best parameters...\")\n",
    "final_train_dataset = MatrixDataset(X_trainval, y_trainval)\n",
    "final_test_dataset = MatrixDataset(X_test, y_test)\n",
    "\n",
    "final_train_loader = DataLoader(final_train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "final_test_loader = DataLoader(final_test_dataset, batch_size=best_params['batch_size'])\n",
    "\n",
    "final_model = CNNClassifier(\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    conv_channels=best_params['conv_channels'],\n",
    "    fc_units=best_params['fc_units'],\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    ")\n",
    "\n",
    "# Initialize wandb with a meaningful run name\n",
    "run_name = f\"Final_Training_lr{best_params['learning_rate']}_conv{best_params['conv_channels']}_fc{best_params['fc_units']}_batch{best_params['batch_size']}_dropout{best_params['dropout_rate']}\"\n",
    "wandb.init(project=\"inverse_problem\", name=run_name)\n",
    "\n",
    "final_trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    logger=WandbLogger(project=\"inverse_problem\"),\n",
    "    accelerator='auto'\n",
    ")\n",
    "\n",
    "# Train the final model\n",
    "final_trainer.fit(final_model, final_train_loader)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = final_trainer.test(final_model, final_test_loader)\n",
    "\n",
    "# Log final test accuracy to wandb\n",
    "wandb.log({\"final_test_accuracy\": test_results[0]['test_acc']})\n",
    "\n",
    "print(f\"\\nFinal test accuracy: {test_results[0]['test_acc']:.4f}\")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"\\nFinished wandb run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse_problem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
