{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from getpass import getpass\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from model import CNNClassifier, MatrixDataset, get_feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = \"./data/\"\n",
    "all_C = np.load(data_path + 'all_C.npy') # 1329 input matrices of size 30x30\n",
    "nMF_label = np.load(data_path + 'nMF_label.npy') # 1329 output labels (0 or 1)\n",
    "\n",
    "class_0_C = all_C[nMF_label == 0] # input matrices of size 30x30 with nMF label 0\n",
    "class_1_C = all_C[nMF_label == 1] # input matrices of size 30x30 with nMF label 1\n",
    "\n",
    "# print some stats\n",
    "print(f\"Number of input matrices with nMF label 0: {class_0_C.shape[0]}\")\n",
    "print(f\"Number of input matrices with nMF label 1: {class_1_C.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the normality of the data\n",
    "non_normal_indices = []\n",
    "\n",
    "# Test each matrix individually\n",
    "for i in range(len(all_C)):\n",
    "    # Flatten current matrix\n",
    "    matrix_flat = all_C[i].flatten()\n",
    "    \n",
    "    # Perform normality test\n",
    "    _, p_value = stats.normaltest(matrix_flat)\n",
    "    \n",
    "    # If p-value <= 0.05, the distribution is not normal\n",
    "    if p_value <= 0.05:\n",
    "        non_normal_indices.append(i)\n",
    "\n",
    "# Print results\n",
    "if len(non_normal_indices) == 0:\n",
    "    print(\"Normality check passed: All matrices are normally distributed\")\n",
    "else:\n",
    "    print(f\"Normality check failed: {len(non_normal_indices)} out of {len(all_C)} matrices are not normally distributed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the matrices\n",
    "plt.figure(figsize=(25, 20))\n",
    "\n",
    "# Plot 10 random matrices from class 0 (in rows 1-2)\n",
    "for i in range(10):\n",
    "    row = i // 5  # 0 for first 5 items, 1 for next 5\n",
    "    col = i % 5   # 0-4 for each row\n",
    "    plt.subplot(4, 5, row*5 + col + 1)\n",
    "    random_idx = np.random.randint(0, len(class_0_C))\n",
    "    im = plt.imshow(class_0_C[random_idx], cmap='viridis')\n",
    "    plt.title(f'Class 0\\nSample {random_idx}')\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "\n",
    "# Plot 10 random matrices from class 1 (in rows 3-4)\n",
    "for i in range(10):\n",
    "    row = i // 5  # 0 for first 5 items, 1 for next 5\n",
    "    col = i % 5   # 0-4 for each row\n",
    "    plt.subplot(4, 5, (row+2)*5 + col + 1)  # +2 to start from third row\n",
    "    random_idx = np.random.randint(0, len(class_1_C))\n",
    "    im = plt.imshow(class_1_C[random_idx], cmap='viridis')\n",
    "    plt.title(f'Class 1\\nSample {random_idx}')\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "\n",
    "# Add a main title\n",
    "plt.suptitle('Random Samples of Correlation Matrices from Both Classes', \n",
    "             fontsize=16, y=1.02)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate min and max for each matrix\n",
    "mins = np.array([matrix.min() for matrix in all_C])\n",
    "maxs = np.array([matrix.max() for matrix in all_C])\n",
    "\n",
    "# Create figure with 3 rows (all data + each class) and 2 columns\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Row 1: All data\n",
    "plt.subplot(321)\n",
    "plt.boxplot([mins, maxs], labels=['Min Values', 'Max Values'])\n",
    "plt.title('All Data: Distribution of Min/Max Values')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "plt.subplot(322)\n",
    "sns.kdeplot(data=mins, label='Min Values', fill=True, alpha=0.3)\n",
    "sns.kdeplot(data=maxs, label='Max Values', fill=True, alpha=0.3)\n",
    "plt.title('All Data: Density Distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# Row 2 & 3: Plot for each class\n",
    "for idx, class_label in enumerate([0, 1]):\n",
    "    class_indices = np.where(nMF_label == class_label)[0]\n",
    "    class_mins = mins[class_indices]\n",
    "    class_maxs = maxs[class_indices]\n",
    "    \n",
    "    # Box plot for class\n",
    "    plt.subplot(323 + idx*2)\n",
    "    plt.boxplot([class_mins, class_maxs], labels=['Min Values', 'Max Values'])\n",
    "    plt.title(f'Class {class_label}: Distribution of Min/Max Values')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    # Density plot for class\n",
    "    plt.subplot(324 + idx*2)\n",
    "    sns.kdeplot(data=class_mins, label='Min Values', fill=True, alpha=0.3)\n",
    "    sns.kdeplot(data=class_maxs, label='Max Values', fill=True, alpha=0.3)\n",
    "    plt.title(f'Class {class_label}: Density Distribution')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics for all data and by class\n",
    "print(\"Overall Statistics:\")\n",
    "print(\"\\nMinimum values:\")\n",
    "print(f\"Mean: {np.mean(mins):.4f}, Std: {np.std(mins):.4f}\")\n",
    "print(f\"Range: [{np.min(mins):.4f}, {np.max(mins):.4f}]\")\n",
    "\n",
    "print(\"\\nMaximum values:\")\n",
    "print(f\"Mean: {np.mean(maxs):.4f}, Std: {np.std(maxs):.4f}\")\n",
    "print(f\"Range: [{np.min(maxs):.4f}, {np.max(maxs):.4f}]\")\n",
    "\n",
    "# Print statistics by class\n",
    "for class_label in [0, 1]:\n",
    "    class_indices = np.where(nMF_label == class_label)[0]\n",
    "    class_mins = mins[class_indices]\n",
    "    class_maxs = maxs[class_indices]\n",
    "    \n",
    "    print(f\"\\nClass {class_label} Statistics:\")\n",
    "    print(f\"Number of samples: {len(class_indices)}\")\n",
    "    print(\"\\nMinimum values:\")\n",
    "    print(f\"Mean: {np.mean(class_mins):.4f}, Std: {np.std(class_mins):.4f}\")\n",
    "    print(f\"Range: [{np.min(class_mins):.4f}, {np.max(class_mins):.4f}]\")\n",
    "    \n",
    "    print(\"\\nMaximum values:\")\n",
    "    print(f\"Mean: {np.mean(class_maxs):.4f}, Std: {np.std(class_maxs):.4f}\")\n",
    "    print(f\"Range: [{np.min(class_maxs):.4f}, {np.max(class_maxs):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get my API key from environment variable first\n",
    "wandb_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "# If not found in environment, prompt user\n",
    "if not wandb_key:\n",
    "    wandb_key = getpass(\"Enter your Weights & Biases API key: \")\n",
    "\n",
    "wandb.login(key=wandb_key)\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-3, 1e-4],\n",
    "    'conv_channels': [[1, 16, 32, 64], [1, 32, 64, 128]],\n",
    "    'dropout_rate': [0.3, 0.5],\n",
    "}\n",
    "\n",
    "# K-fold setup\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid search with k-fold cross validation\n",
    "best_val_acc = 0\n",
    "best_params = None\n",
    "\n",
    "for lr in param_grid['learning_rate']:\n",
    "    for conv_ch in param_grid['conv_channels']:\n",
    "        for dropout in param_grid['dropout_rate']:\n",
    "            fold_scores = []\n",
    "            \n",
    "            # Initialize a new wandb run\n",
    "            run = wandb.init(\n",
    "                project=\"inverse_problem\",\n",
    "                config={\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"conv_channels\": conv_ch,\n",
    "                    \"dropout_rate\": dropout,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(kfold.split(all_C)):\n",
    "                # Prepare data\n",
    "                train_dataset = MatrixDataset(all_C[train_idx], nMF_label[train_idx])\n",
    "                val_dataset = MatrixDataset(all_C[val_idx], nMF_label[val_idx])\n",
    "                \n",
    "                train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "                \n",
    "                # Initialize model and trainer\n",
    "                model = CNNClassifier(\n",
    "                    learning_rate=lr,\n",
    "                    conv_channels=conv_ch,\n",
    "                    dropout_rate=dropout\n",
    "                )\n",
    "                \n",
    "                checkpoint_callback = ModelCheckpoint(\n",
    "                    monitor='val_acc',\n",
    "                    dirpath='checkpoints',\n",
    "                    filename=f'fold_{fold}',\n",
    "                    save_top_k=1,\n",
    "                    mode='max'\n",
    "                )\n",
    "                \n",
    "                trainer = pl.Trainer(\n",
    "                    max_epochs=50,\n",
    "                    logger=WandbLogger(project=\"inverse_problem\"),\n",
    "                    callbacks=[checkpoint_callback],\n",
    "                    accelerator='auto'\n",
    "                )\n",
    "                \n",
    "                # Train and validate\n",
    "                trainer.fit(model, train_loader, val_loader)\n",
    "                \n",
    "                # Store fold score\n",
    "                fold_scores.append(checkpoint_callback.best_model_score.item())\n",
    "            \n",
    "            # Calculate average score for this parameter combination\n",
    "            avg_score = np.mean(fold_scores)\n",
    "            if avg_score > best_val_acc:\n",
    "                best_val_acc = avg_score\n",
    "                best_params = {\n",
    "                    'learning_rate': lr,\n",
    "                    'conv_channels': conv_ch,\n",
    "                    'dropout_rate': dropout\n",
    "                }\n",
    "            \n",
    "            wandb.finish()\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best validation accuracy:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature map visualization\n",
    "def visualize_feature_maps(feature_maps, sample_idx=0):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i, (name, features) in enumerate(feature_maps.items()):\n",
    "        # Get the first sample's feature maps\n",
    "        feature_map = features[sample_idx].cpu().numpy()\n",
    "        \n",
    "        # Create subplot for each conv layer\n",
    "        n_features = min(4, feature_map.shape[0])  # Show up to 4 features\n",
    "        for j in range(n_features):\n",
    "            plt.subplot(len(feature_maps), 4, i*4 + j + 1)\n",
    "            plt.imshow(feature_map[j], cmap='viridis')\n",
    "            plt.axis('off')\n",
    "            if j == 0:\n",
    "                plt.title(f'{name}\\nChannel {j}')\n",
    "            else:\n",
    "                plt.title(f'Channel {j}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load best model and get feature maps\n",
    "best_model = CNNClassifier.load_from_checkpoint('checkpoints/best_model.ckpt')\n",
    "sample_input = torch.FloatTensor(all_C[:1]).unsqueeze(1)  # Get first sample\n",
    "feature_maps = get_feature_maps(best_model, sample_input)\n",
    "visualize_feature_maps(feature_maps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse_problem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
